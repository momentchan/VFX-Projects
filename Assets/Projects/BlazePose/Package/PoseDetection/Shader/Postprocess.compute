/*
PoseDetection neural network model output shape are
	_Scores: (1, 896)
	_Boxes: (12, 896) .

_Boxes variable's 12 rows are
	position, size,
	hip center position, fully body ROI,
	sholder center postion, upper body ROI.
	(https://github.com/google/mediapipe/issues/1449#issuecomment-770124530)

	896 Colmuns are vectors flatten anchors feture map(6*8*8 + 2*16*16).
	Anchors feture map Similar to Mediapipe BlazeFace(https://arxiv.org/pdf/1907.05047.pdf).
*/

#pragma kernel ProcessForMap8
#pragma kernel ProcessForMap16

#include "PoseData.cginc"
#define IMAGE_SIZE 128

float _Threshold;
Texture2D<float> _Scores;
Texture2D<float> _Boxes;
AppendStructuredBuffer<PoseData> _Output;

float Sigmoid(float x) {
	return 1.0 / (1.0 + exp(-x));
}

void PostProcess(uint2 id, uint mapSize, uint chSize, uint id_offset) {

	const float scale = 1.0 / IMAGE_SIZE;
	uint index_In0chMap = (id.y * mapSize + id.x) * chSize + id_offset;
	float2 anchor = (mapSize - id - 0.5) / mapSize;

	for (uint i = 0; i < chSize; i++) {
		uint index = index_In0chMap + i;

		PoseData pd;
		pd.score = Sigmoid(_Scores[uint2(0, index)]);

		float x = _Boxes[uint2(0, index)] * scale;
		float y = _Boxes[uint2(1, index)] * scale;
		float w = _Boxes[uint2(2, index)] * scale;
		float h = _Boxes[uint2(3, index)] * scale;
		pd.center = anchor + float2(x, y);
		pd.extent = float2(w, h);

		[unroll]
		for (int j = 0; j < 4; j++) {
			x = _Boxes[uint2(4 + 2 * j, index)] * scale;
			y = _Boxes[uint2(5 + 2 * j, index)] * scale;
			pd.keyPoints[j] = anchor + float2(x, y);
		}

		if (pd.score > _Threshold)
			_Output.Append(pd);
	}
}

[numthreads(8, 8, 1)]
void ProcessForMap8(uint2 id : SV_DispatchThreadID) {
	PostProcess(id, 8, 6, 0);
}

[numthreads(16, 16, 1)]
void ProcessForMap16(uint2 id : SV_DispatchThreadID) {
	const uint offset = 8 * 8 * 6;
	PostProcess(id, 16, 2, offset);
}
